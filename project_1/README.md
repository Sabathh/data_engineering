# Project: Data Modeling with Postgres

First project of the Data Engineering Nanodegree.

## Project description

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis, and bring you on the project. Your role is to create a database schema and ETL pipeline for this analysis. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

## How to Execute

- Create a PostgreSQL database and user according to the following: dbname=studentdb user=student password=student *(Alternatively, modify create_tables.py to the database and user you'd like to use)*
- Run `create_tables.py` to create the tables used by the ETL (it also drops previously existing tables to make sure the tables created are empty)
- Run `etl.py` to import the files from the `data` folder into the database
- Run `test.ipynb` to confirm the data was successfully imported into the tables

*Note: If you see a similar error while running `create_tables.py`:*

```text
    DETAIL:  There are 2 other sessions using the database.
```

*Open the SQL Shell and run the following command to close all sessions:*

```SQL
    select pg_terminate_backend(pid) from pg_stat_activity where datname='sparkifydb';
```

## Datasets

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. Here's an example of the format of a single song file:

```json
    {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

Here's an example of the format of log file:

![Example of Log Dataset](images\log-data.png "Example of Log Dataset")

## Database Schemas

The schema model selected for this exercise is the Star Schema. It contains 1 Fact Table with the measures associated to each event, and 4 Dimensional Tables, each with a Primary Key referenced from the Fact Table.

### Fact Table

#### songplays

- songplay_id (int PRIMARY KEY) : Unique ID of each song played
- start_time (float) : Timestamp of beginning of user activity
- user_id (int) : Unique User ID
- level (varchar) : User level (free/paid)
- song_id (varchar) : Unique song ID
- artist_id (varchar) : Unique artist ID
- session_id (int) : Unique user session ID
- location (varchar) : User location
- user_agent (varchar) : Platform used by the user to access service

### Dimension Tables

#### users

- user_id (int PRIMARY KEY) : Unique User ID
- first_name (varchar) : User's first name
- last_name (varchar) : User's last name
- gender (varchar) : User's gender
- level (varchar) : User level (free/paid)

#### songs

- song_id (varchar PRIMARY KEY) : Unique song ID
- title (varchar) : Song title
- artist_id (varchar) : Unique artist ID
- year (int) : Year the song was released
- duration (float) : Song duration

#### artists

- artist_id (varchar PRIMARY KEY) : Unique artist ID
- name (varchar) : Artist's name
- location (varchar) : Artist's city
- latitude (float) : Artist's latitude
- longitude (float) : Artist's longitude

#### time

- start_time (float PRIMARY KEY) : Timestamp of beginning of user activity
- hour (int) : Hour related to start_time
- day (int) : Day related to start_time
- week (int) : Week related to start_time
- month (int) : Month related to start_time
- year (int) : Year related to start_time
- weekday (varchar) : Weekday related to start_time
