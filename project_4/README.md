# Project: Data Lake

Fourth project of the Data Engineering Nanodegree.

## Project description

A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

The task is to build an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

## How to execute

### Configuration

The file [dl.cfg](https://github.com/Sabathh/data_engineering/blob/master/project_4/dl.cfg) must be populated with AWS's credentials. 

(Tip: do not write the credentials in-between quotes)

```text
[AWS]
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
```

### Populating Datalake

Running [etl.py](https://github.com/Sabathh/data_engineering/blob/master/project_4/etl.py) will automatically process the  input data and load it to the desired output path. Make sure you have access to both input source and output destination before running it. Modify the following in the main function of the file as required:

```python
    input_data = "s3a://udacity-dend/"
    output_data = "s3a://sabathh-us-west-2/"
```

## Datasets

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. Here's an example of the format of a single song file:

```json
    {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

The dataset is available in the following S3 bucket:

```text
s3://udacity-dend/song_data
```

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

Here's an example of the format of log file:

![Example of Log Dataset](https://github.com/Sabathh/data_engineering/blob/master/project_1/images/log-data.png?raw=true "Example of Log Dataset")

The dataset is available in the following S3 bucket:

```text
s3://udacity-dend/log_data
```

## Database Schema

### Fact Table

- _songplays_ - Entries in log data associated with songplays i.e. records with page='NextSong':
  - session_id
  - start_time
  - user_id
  - level
  - song_id
  - artist_id
  - location
  - user_agent
  - year
  - month

### Dimension Tables

- _users_:
  - user_id
  - first_name
  - last_name
  - gender
  - level

- _songs_:
  - song_id
  - title
  - artist_id
  - year
  - duration

- _artists_:
  - artist_id
  - artist_name
  - artist_location
  - artist_latitude
  - artist_longitude

- _time_:
  - timestamp
  - hour
  - day
  - week
  - month
  - year
  - weekday
